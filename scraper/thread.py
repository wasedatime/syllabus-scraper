from concurrent.futures import ThreadPoolExecutor, wait
from scraper import crawler


def run_concurrent(dept):
    # TODO
    # tasks = []
    # with ThreadPoolExecutor() as executor:
    #     for page in range(1, ):
    #         tasks.append(executor.submit(function_here, dept, page))
    # wait(tasks)
    pass
